{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ipynb.fs.full.features import features_independientes_precio, features_dependientes_precio\n",
    "import ipynb.fs.full.utils as utils\n",
    "from hyperopt import fmin, tpe, hp\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrego Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_f = features_independientes_precio(df_train)\n",
    "df_train_f = features_dependientes_precio(df_train_f, df_train)\n",
    "\n",
    "df_test_f = features_independientes_precio(df_test)\n",
    "df_test_f = features_dependientes_precio(df_test_f, df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtro columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['antiguedad', 'habitaciones', 'garages', 'banos', 'metroscubiertos',\n",
    "       'metrostotales', 'idzona', 'lat', 'lng', 'gimnasio', 'usosmultiples',\n",
    "       'piscina', 'escuelascercanas', 'centroscomercialescercanos', 'precio']\n",
    "\n",
    "features_extra = ['prop_frecuente', 'es_ciudad_centrica', 'porcentaje_metros', 'diferencia_metros',\n",
    "       'delincuencia','turismo','promedio_precio_ciudad', 'promedio_precio_tipo_propiedad',\n",
    "       'anio','promedio_metros_cub_tipo_propiedad', 'top_provincia', 'promedio_id_zona', 'promedio_por_mes', \n",
    "       'count_id_zona', 'count_ciudad', 'puntaje', 'count_tipo_propiedad', 'count_tipo_propiedad_ciudad',\n",
    "       'promedio_id_zona_gen', 'promedio_precio_tipo_propiedad_ciudad_gen', 'promedio_precio_hbg_tipo_propiedad_provincia']\n",
    "\n",
    "features += features_extra\n",
    "\n",
    "df_XGBoost, df_eval = utils.dividir_df_testeo(df_train, test_size=0.15)\n",
    "\n",
    "df_XGBoost = utils.filtrar_features(df_XGBoost, features)\n",
    "df_eval = utils.filtrar_features(df_XGBoost, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busco los mejores hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def xgb_eval(args):\n",
    "    \n",
    "    # Modelo - Ver hiperparametros\n",
    "    colsample_bytree, learning_rate, max_depth, alpha, n_estimators, test_size = args\n",
    "    \n",
    "    # Preparacion de los datos\n",
    "    x_train, x_test, y_train, y_test = utils.dividir_dataset(df_XGBoost, 'precio', features, test_size=test_size)\n",
    "    \n",
    "    x_eval = df_eval.drop('precio', axis=1)\n",
    "    y_eval = df_eval['precio']\n",
    "    \n",
    "    xg_train = xgb.DMatrix(x_train, label=y_train)\n",
    "    xg_test = xgb.DMatrix(x_test, label=y_test)\n",
    "    dfg_test = xgb.DMatrix(x_eval, label = y_eval)\n",
    "\n",
    "    max_depth = int(max_depth)\n",
    "    n_estimators = int(n_estimators)\n",
    "    params = {\n",
    "        'objective':'reg:squarederror', \n",
    "        'colsample_bytree' : colsample_bytree , \n",
    "        'learning_rate' : learning_rate,\n",
    "        'max_depth' : max_depth, \n",
    "        'alpha' : alpha,\n",
    "        'n_estimators' : n_estimators,\n",
    "        'eval_metric': 'mae'\n",
    "    }\n",
    "\n",
    "    watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "    num_round = 25\n",
    "    \n",
    "    xg_reg = xgb.train(params, \n",
    "                    xg_train, \n",
    "                    num_round, \n",
    "#                     watchlist\n",
    "                   )\n",
    "\n",
    "    y_pred_eval = xg_reg.predict(dfg_test)\n",
    "    y_pred_test = xg_reg.predict(xg_test)\n",
    "    return utils.MAE(y_test, y_pred_test)\n",
    "\n",
    "space = [hp.quniform('colsample_bytree', 0.3, 1, 0.05), hp.quniform('learning_rate', 0.01, 0.15, 0.01),\n",
    "         hp.quniform(\"max_depth\", 1, 20, 1),hp.uniform(\"alpha\", 0.01, 30),\n",
    "        hp.quniform(\"n_estimators\", 100, 600, 30), hp.quniform(\"test_size\", 0.1, 0.4, 0.05)]\n",
    "\n",
    "hps = fmin(xgb_eval, space=space, algo=tpe.suggest, max_evals=50)\n",
    "\n",
    "display(hps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Busco los mejores features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['antiguedad', 'habitaciones', 'garages', 'banos',\n",
    "       'metroscubiertos', 'metrostotales', \n",
    "            # 'lat', 'lng',\n",
    "       'gimnasio', 'usosmultiples', 'piscina', 'escuelascercanas', 'centroscomercialescercanos']\n",
    "\n",
    "features_test = ['tipo_propiedad_compartida',\n",
    "                 'prop_frecuente', 'top_provincia', 'es_ciudad_centrica',\n",
    "                 'promedio_metros_totales_provincia',\n",
    "                 'promedio_metros_cubiertos_provincia', 'porcentaje_metros',\n",
    "                 'diferencia_metros', 'delincuencia', 'turismo',\n",
    "                 'promedio_precio_ciudad', 'promedio_id_zona',\n",
    "                 'promedio_precio_tipo_propiedad','es_antigua',\n",
    "                 'promedio_por_mes', 'promedio_precio_habitaciones',\n",
    "                 'promedio_precio_habitaciones_banos_garages','cantidad_inquilinos',\n",
    "                 'promedio_precio_banos_garages', 'promedio_precio_booleanos',\n",
    "                 'metros_totales_normalizados', 'metros_cubiertos_normalizados','escomercial',\n",
    "                 'promedio_metros_cub_tipo_propiedad', 'anio', 'mes', 'dia','trimestre']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hps = {'alpha': 9.499855511978337,\n",
    " 'colsample_bytree': 0.8500000000000001,\n",
    " 'learning_rate': 0.2226513740962932,\n",
    " 'max_depth': 20.0,\n",
    " 'n_estimators': 240.0,\n",
    " 'test_size': 0.103950169925154826\n",
    "}\n",
    "\n",
    "alpha = hps['alpha']\n",
    "colsample_bytree = hps['colsample_bytree']\n",
    "max_depth = hps['max_depth']\n",
    "learning_rate = hps['learning_rate']\n",
    "n_estimators = hps['n_estimators']\n",
    "\n",
    "n_estimators = int(hps['n_estimators'])\n",
    "max_depth = int(hps['max_depth'])\n",
    "\n",
    "params = {\n",
    "        'objective':'reg:squarederror', \n",
    "        'colsample_bytree' : colsample_bytree , \n",
    "        'learning_rate' : learning_rate,\n",
    "        'max_depth' : max_depth, \n",
    "        'alpha' : alpha,\n",
    "        'n_estimators' : n_estimators,\n",
    "        'eval_metric': 'mae'\n",
    "}\n",
    "\n",
    "\n",
    "base_train = 0\n",
    "base_test = 0\n",
    "base_eval = 0\n",
    "for i in ['None'] + features_test:\n",
    "    x_train, x_test, y_train, y_test = utils.dividir_dataset(df_train_f, 'precio', features + [i])\n",
    "\n",
    "    xg_train = xgb.DMatrix(x_train, label=y_train)\n",
    "    xg_test = xgb.DMatrix(x_test, label=y_test)\n",
    "    xg_train = xgb.DMatrix(x_train, label=y_train)\n",
    "    xg_test = xgb.DMatrix(x_test, label=y_test)\n",
    "\n",
    "    watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "    num_round = 10\n",
    "    \n",
    "    xg_reg = xgb.train(params, \n",
    "                    xg_train, \n",
    "                    num_round, \n",
    "                    watchlist\n",
    "                   )\n",
    "    \n",
    "    x_train_predict = xgb.DMatrix(x_train, label=y_train)\n",
    "    x_test_predict = xgb.DMatrix(x_test, label=y_test)\n",
    "    x_eval_predict = xgb.DMatrix(utils.filtrar_features(df_eval, features + [i]))\n",
    "    \n",
    "    y_pred_test = xg_reg.predict(x_test_predict)\n",
    "    y_pred_train = xg_reg.predict(x_train_predict)\n",
    "    y_pred_eval = xg_reg.predict(x_eval_predict)\n",
    "\n",
    "    xgb_mae_train = utils.MAE(y_train, y_pred_train)\n",
    "    xgb_mae = utils.MAE(y_test, y_pred_test)\n",
    "    xgb_mae_eval = utils.MAE(df_eval['precio'].values, y_pred_eval)\n",
    "\n",
    "    print(f\"MAE LightGBM (train): {xgb_mae_train:.5f}\")\n",
    "    print(f\"MAE LightGBM (test): {xgb_mae:.5f}\")\n",
    "    print(f\"MAE LightGBM (eval): {xgb_mae_eval:.5f}\")\n",
    "    if i is not 'None':\n",
    "        print(f\"Overfitting (base_eval - base_test) - (eval - test) - {i}: {(base_eval - base_test) - (xgb_mae_eval - xgb_mae)}\")\n",
    "        print(f\"Diff evaluation (base_eval - eval)                  - {i}: {base_eval - xgb_mae_eval}\")\n",
    "        print(f\"Diff train (base_train - train)                     - {i}: {base_train - xgb_mae_train}\")\n",
    "    else:\n",
    "        base_train = xgb_mae_train\n",
    "        base_test = xgb_mae\n",
    "        base_eval = xgb_mae_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = ['antiguedad', 'habitaciones', 'garages', 'banos', 'metroscubiertos',\n",
    "       'metrostotales', 'idzona', 'lat', 'lng', 'gimnasio', 'usosmultiples',\n",
    "       'piscina', 'escuelascercanas', 'centroscomercialescercanos', 'precio']\n",
    "\n",
    "features_extra = ['prop_frecuente', 'es_ciudad_centrica', 'porcentaje_metros', 'diferencia_metros',\n",
    "       'delincuencia','turismo','promedio_precio_ciudad', 'promedio_precio_tipo_propiedad',\n",
    "       'anio','promedio_metros_cub_tipo_propiedad', 'top_provincia', 'promedio_id_zona', 'promedio_por_mes', \n",
    "       'count_id_zona', 'count_ciudad', 'puntaje', 'count_tipo_propiedad', 'count_tipo_propiedad_ciudad']\n",
    "\n",
    "features += features_extra\n",
    "\n",
    "\n",
    "# Preparación hiperparámetros\n",
    "\n",
    "# hps = {'alpha': 9.499855511978337,\n",
    "#  'colsample_bytree': 0.8500000000000001,\n",
    "#  'learning_rate': 0.2226513740962932,\n",
    "#  'max_depth': 20.0,\n",
    "#  'n_estimators': 240.0,\n",
    "#  'test_size': 0.103950169925154826\n",
    "# }\n",
    "hps = {'alpha': 12.338515720263425,\n",
    " 'colsample_bytree': 0.8500000000000001,\n",
    " 'learning_rate': 0.15,\n",
    " 'max_depth': 17.0,\n",
    " 'n_estimators': 600.0,\n",
    " 'test_size': 0.15000000000000002}\n",
    "\n",
    "alpha = hps['alpha']\n",
    "colsample_bytree = hps['colsample_bytree']\n",
    "max_depth = hps['max_depth']\n",
    "learning_rate = hps['learning_rate']\n",
    "n_estimators = hps['n_estimators']\n",
    "test_size = hps['test_size']\n",
    "\n",
    "n_estimators = int(hps['n_estimators'])\n",
    "max_depth = int(hps['max_depth'])\n",
    "\n",
    "\n",
    "# Preparacion de los datos\n",
    "x_train, x_test, y_train, y_test = utils.dividir_dataset(df_XGBoost, 'precio', features, test_size=test_size)\n",
    "\n",
    "x_eval = df_eval.drop('precio', axis=1)\n",
    "y_eval = df_eval['precio']\n",
    "\n",
    "xg_train = xgb.DMatrix(x_train, label=y_train)\n",
    "xg_test = xgb.DMatrix(x_test, label=y_test)\n",
    "dfg_test = xgb.DMatrix(x_eval, label = y_eval)\n",
    "\n",
    "\n",
    "params = {\n",
    "    'objective':'reg:squarederror', \n",
    "    'colsample_bytree' : colsample_bytree , \n",
    "    'learning_rate' : learning_rate,\n",
    "    'max_depth' : max_depth, \n",
    "    'alpha' : alpha,\n",
    "    'n_estimators' : n_estimators,\n",
    "    'eval_metric': 'mae'\n",
    "}\n",
    "\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "\n",
    "# Entrenamiento\n",
    "xg_reg = xgb.train(params, \n",
    "                   xg_train, \n",
    "                   num_boost_round=50,\n",
    "                   evals=watchlist)\n",
    "\n",
    "# Prediccion\n",
    "y_pred_train = xg_reg.predict(xg_train)\n",
    "y_pred_test = xg_reg.predict(xg_test)\n",
    "\n",
    "\n",
    "linear_mae_train = utils.MAE(y_train, y_pred_train)\n",
    "linear_mae = utils.MAE(y_test, y_pred_test)\n",
    "\n",
    "y_pred_eval = xg_reg.predict(dfg_test)\n",
    "linear_mae_eval = utils.MAE(y_eval, y_pred_eval)\n",
    "\n",
    "\n",
    "print(f\"MAE (train): {linear_mae_train:.5f}\")\n",
    "print(f\"MAE: {linear_mae:.5f}\")\n",
    "print(f\"MAE (eval): {linear_mae_eval:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_final = pd.read_csv('./data/train.csv')\n",
    "df_test_final = pd.read_csv('./data/test.csv')\n",
    "\n",
    "df_train_final_f = features_independientes_precio(df_train_final)\n",
    "df_train_final_f = features_dependientes_precio(df_train_final_f, df_train_final)\n",
    "df_test_final_f = features_independientes_precio(df_test_final)\n",
    "df_test_final_f = features_dependientes_precio(df_test_final_f, df_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = ['antiguedad', 'habitaciones', 'garages', 'banos', 'metroscubiertos',\n",
    "       'metrostotales', 'idzona', 'lat', 'lng', 'gimnasio', 'usosmultiples',\n",
    "       'piscina', 'escuelascercanas', 'centroscomercialescercanos', 'precio']\n",
    "\n",
    "features_extra = ['prop_frecuente', 'es_ciudad_centrica', 'porcentaje_metros', 'diferencia_metros',\n",
    "       'delincuencia','turismo','promedio_precio_ciudad', 'promedio_precio_tipo_propiedad',\n",
    "       'anio','promedio_metros_cub_tipo_propiedad', 'top_provincia', 'promedio_id_zona', 'promedio_por_mes', \n",
    "       'count_id_zona', 'count_ciudad', 'puntaje', 'count_tipo_propiedad', 'count_tipo_propiedad_ciudad',\n",
    "       'promedio_id_zona_gen', 'promedio_precio_tipo_propiedad_ciudad_gen', 'promedio_precio_hbg_tipo_propiedad_provincia']\n",
    "\n",
    "features += features_extra\n",
    "\n",
    "# Filtro columnas\n",
    "df_XGBoost_fin = utils.filtrar_features(df_train_final_f, features)\n",
    "df_eval_fin = utils.filtrar_features(df_test_final_f, features)\n",
    "\n",
    "x_train_f, x_test_f, y_train_f, y_test_f = utils.dividir_dataset(df_XGBoost_fin, 'precio', features)\n",
    "\n",
    "\n",
    "xg_train = xgb.DMatrix(x_train_f, label=y_train_f)\n",
    "xg_test = xgb.DMatrix(x_test_f, label=y_test_f)\n",
    "\n",
    "# Modelo - Ver hiperparametros\n",
    "params = {\n",
    "    'objective':'reg:squarederror', \n",
    "    'colsample_bytree' : colsample_bytree , \n",
    "    'learning_rate' : learning_rate,\n",
    "    'max_depth' : max_depth, \n",
    "    'alpha' : alpha,\n",
    "    'n_estimators' : n_estimators,\n",
    "    'eval_metric': 'mae'\n",
    "}\n",
    "\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "\n",
    "# Entrenamiento\n",
    "xg_reg = xgb.train(params, \n",
    "                   xg_train, \n",
    "                   num_boost_round=2000,\n",
    "                   evals=watchlist\n",
    "                  )\n",
    "\n",
    "\n",
    "# Prediccion\n",
    "y_pred_test_f = xg_reg.predict(xgb.DMatrix(x_test_f))\n",
    "y_pred_train_f = xg_reg.predict(xgb.DMatrix(x_train_f))\n",
    "\n",
    "linear_mae_train_f = utils.MAE(y_train_f, y_pred_train_f)\n",
    "linear_mae_f = utils.MAE(y_test_f, y_pred_test_f)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"MAE (train): {linear_mae_train_f:.5f}\")\n",
    "print(f\"MAE: {linear_mae_f:.5f}\")\n",
    "\n",
    "prediccion_final = xg_reg.predict(xgb.DMatrix(df_eval_fin))\n",
    "df_test_final_f['target'] = prediccion_final\n",
    "df_test_final_f[['id', 'target']].to_csv('respuesta2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificación de Nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_final_f['target'].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_final_f['target'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_f.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
