{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp\n",
    "import ipynb.fs.full.utils as utils\n",
    "import ipynb.fs.full.features as features\n",
    "import ipynb.fs.full.features_distancias as f_distancias\n",
    "\n",
    "df_train = pd.read_csv('./data/train_filtrado.csv')\n",
    "# Para usarse con el submit a Kaggle\n",
    "df_test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "df_train = features.llenar_nulls(df_train)\n",
    "df_test = features.llenar_nulls(df_test, hgb_mean=True, df_fill=df_train)\n",
    "\n",
    "# df_train, df_test = features_de_csvs(df_train, df_test)\n",
    "\n",
    "# df_train, df_test = utils.dividir_df_testeo(df_train, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cluster = pd.read_csv('./data/clustering_train.csv').rename(columns={'label': 'clustering_label'})\n",
    "df_test_cluster = pd.read_csv('./data/clustering_test.csv').rename(columns={'label': 'clustering_label'})\n",
    "\n",
    "df_train = pd.merge(df_train, df_train_cluster, on='id')\n",
    "df_test = pd.merge(df_test, df_test_cluster, on='id')\n",
    "\n",
    "df_train_idf = pd.read_csv('./data/train_idf.csv')\n",
    "df_test_idf = pd.read_csv('./data/test_idf.csv')\n",
    "\n",
    "df_train = pd.merge(df_train, df_train_idf, on= 'id', how= 'left')\n",
    "df_test = pd.merge(df_test, df_test_idf, on= 'id', how= 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train, df_test = utils.dividir_df_testeo(df_train, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_f = features.features_independientes_precio(df_test)\n",
    "df_test_f = features.features_dependientes_precio(df_test_f, df_train)\n",
    "\n",
    "df_train_f = features.features_independientes_precio(df_train)\n",
    "df_train_f = features.features_dependientes_precio(df_train_f, df_train)\n",
    "\n",
    "df_test_f, cols_tipodepropiedad_ohe = features.columna_a_ohe(df_test_f, 'tipodepropiedad', N=100, df_aux=df_train, devolver_cols=True)\n",
    "df_test_f, cols_provincia_ohe = features.columna_a_ohe(df_test_f, 'provincia', N=100, df_aux=df_train, devolver_cols=True)\n",
    "df_test_f, cols_zona_ohe = features.columna_a_ohe(df_test_f, 'zona', df_aux=df_train_f, devolver_cols=True)\n",
    "\n",
    "df_train_f = features.columna_a_ohe(df_train_f, 'tipodepropiedad', N=100, df_aux=df_test)\n",
    "df_train_f = features.columna_a_ohe(df_train_f, 'provincia', N=100, df_aux=df_test)\n",
    "df_train_f = features.columna_a_ohe(df_train_f, 'zona', df_aux=df_test_f)\n",
    "\n",
    "\n",
    "df_train_f['fecha'] = pd.to_datetime(df_train_f['fecha']).astype(int)\n",
    "df_test_f['fecha'] = pd.to_datetime(df_test_f['fecha']).astype(int)\n",
    "\n",
    "\n",
    "df_train_f = f_distancias.feature_distancias(df_train_f)\n",
    "df_test_f = f_distancias.feature_distancias(df_test_f, df_train_f)\n",
    "\n",
    "\n",
    "# df_train_f = features.KD_feature(df_train_f)\n",
    "# df_test_f =  features.KD_feature(df_test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class LightGBMWrapper(lgb.LGBMRegressor):\n",
    "    \n",
    "    def fit(self, x, y):        \n",
    "        return super(LightGBMWrapper, self).fit(x, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return super(LightGBMWrapper, self).predict(X,num_iteration=self.best_iteration_)\n",
    "\n",
    "hps = {'bagging_fraction': 0.5,\n",
    " 'boosting_type': 'gbdt',\n",
    " 'feature_fraction': 0.9,\n",
    " 'learning_rate': 0.25,\n",
    " 'max_depth': 10,\n",
    " 'metric': 'mae',\n",
    " 'n_jobs': 2,\n",
    " 'num_leaves': 200,\n",
    " 'objective': 'regression'}\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae', # Si se deja vacio se toma el ideal para llegar al 'objective'\n",
    "    'num_leaves': int(hps['num_leaves']),\n",
    "    'learning_rate': hps['learning_rate'],\n",
    "    'feature_fraction': hps['feature_fraction'],\n",
    "    'bagging_fraction': hps['bagging_fraction'],\n",
    "#     'bagging_freq': int(hps['bagging_freq']),\n",
    "    'max_depth': int(hps['max_depth']),\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "lgb_m = LightGBMWrapper(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "\n",
    "def keras_modelo():    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=200, activation='selu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(units=200, activation='selu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(units=200, activation='selu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_squared_error'])  \n",
    "    return model\n",
    "\n",
    "keras_m = KerasRegressor(build_fn=keras_modelo, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "class XGBoostWrapper(xgb.XGBRegressor):\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        return super(xgb.XGBRegressor, self).fit(x, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return super(xgb.XGBRegressor, self).predict(X)\n",
    "\n",
    "hps = {'colsample_bytree': 0.9,\n",
    " 'eval_metric': 'mae',\n",
    " 'learning_rate': 0.1,\n",
    " 'max_depth': 10,\n",
    " 'n_estimators': 120,\n",
    " 'n_jobs': 4,\n",
    " 'objective': 'reg:squarederror',\n",
    " 'scale_pos_weight': 1,\n",
    " 'verbosity': 0}\n",
    "\n",
    "\n",
    "# n_estimators = int(hps['n_estimators'])\n",
    "# max_depth = int(hps['max_depth'])\n",
    "\n",
    "xgb_m = XGBoostWrapper(**hps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "params = {'bootstrap': False,\n",
    "          'max_features': 'sqrt',\n",
    "          'min_samples_split': 4,\n",
    "          'n_jobs': 2}\n",
    "\n",
    "forest_m = RandomForestRegressor(n_estimators=100, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "params = {'criterion': 'mse',\n",
    " 'max_features': 'sqrt',\n",
    " 'min_samples_split': 4}\n",
    "\n",
    "extratrees_m = ExtraTreesRegressor(n_estimators=50, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [regression]\n",
      "metric:       [mean_absolute_error]\n",
      "variant:      [B]\n",
      "n_estimators: [4]\n",
      "\n",
      "estimator  0: [lightgbm: LightGBMWrapper]\n",
      "    fold  0:  [501478.01739473]\n",
      "    fold  1:  [497414.73277108]\n",
      "    fold  2:  [495426.11090588]\n",
      "    fold  3:  [500568.06879772]\n",
      "    ----\n",
      "    MEAN:     [498721.73246735] + [2427.72702997]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n",
      "estimator  1: [extratrees: ExtraTreesRegressor]\n",
      "    fold  0:  [509672.81794688]\n",
      "    fold  1:  [507053.70398601]\n",
      "    fold  2:  [503436.13110065]\n",
      "    fold  3:  [508618.59928154]\n",
      "    ----\n",
      "    MEAN:     [507195.31307877] + [2361.95238448]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n",
      "estimator  2: [randomforest: RandomForestRegressor]\n",
      "    fold  0:  [477209.60261355]\n",
      "    fold  1:  [473691.24736816]\n",
      "    fold  2:  [472930.47367288]\n",
      "    fold  3:  [476387.60918731]\n",
      "    ----\n",
      "    MEAN:     [475054.73321047] + [1788.26681029]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n",
      "estimator  3: [xgboost: XGBoostWrapper]\n",
      "    fold  0:  [486552.78820370]\n",
      "    fold  1:  [482792.98843684]\n",
      "    fold  2:  [480674.17929459]\n",
      "    fold  3:  [485654.34528671]\n",
      "    ----\n",
      "    MEAN:     [483918.57530546] + [2331.57812947]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from vecstack import StackingTransformer\n",
    "\n",
    "features = ['habitaciones', 'garages','banos','antiguedad', 'metroscubiertos',  'metrostotales','lat_norm', \n",
    "           'lng_norm', 'gimnasio', 'usosmultiples', 'piscina']\n",
    "\n",
    "features_test = ['prop_frecuente', 'top_provincia', 'promedio_precio_ciudad', 'anio', 'promedio_id_zona', \n",
    "                 'promedio_precio_tipo_propiedad', 'count_id_zona', 'count_ciudad', 'puntaje', \n",
    "                 'count_tipo_propiedad_ciudad', 'promedio_precio_tipo_propiedad_ciudad_gen',\n",
    "                 'dias_desde_datos','meses_desde_datos','porcentaje_metros','distancia_ciudad_centrica', \n",
    "                 'clustering_label', 'distancia_centro_mexico'\n",
    "                ]\n",
    "\n",
    "features += features_test \n",
    "\n",
    "x_train, x_test, y_train, y_test = utils.dividir_dataset(df_train_f, 'precio', features, test_size=2)\n",
    "\n",
    "modelos = [\n",
    "           ('lightgbm', lgb_m), \n",
    "           ('extratrees', extratrees_m),\n",
    "           ('randomforest', forest_m),\n",
    "         #   ('keras', keras_m), \n",
    "           ('xgboost', xgb_m)\n",
    "          ]\n",
    "\n",
    "stack = StackingTransformer(modelos, regression=True, verbose=2, n_folds=4, variant='B')\n",
    "\n",
    "stack = stack.fit(x_train, y_train)\n",
    "\n",
    "# s_train = stack.transform(x_train)\n",
    "# s_test = stack.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming...\n",
      "\n",
      "estimator  0: [lightgbm: LightGBMWrapper]\n",
      "    DONE\n",
      "\n",
      "estimator  1: [extratrees: ExtraTreesRegressor]\n",
      "    DONE\n",
      "\n",
      "estimator  2: [randomforest: RandomForestRegressor]\n",
      "    DONE\n",
      "\n",
      "estimator  3: [xgboost: XGBoostWrapper]\n",
      "    DONE\n",
      "\n",
      "Transforming...\n",
      "\n",
      "estimator  0: [lightgbm: LightGBMWrapper]\n",
      "    DONE\n",
      "\n",
      "estimator  1: [extratrees: ExtraTreesRegressor]\n",
      "    DONE\n",
      "\n",
      "estimator  2: [randomforest: RandomForestRegressor]\n",
      "    DONE\n",
      "\n",
      "estimator  3: [xgboost: XGBoostWrapper]\n",
      "    DONE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_train = stack.transform(utils.filtrar_features(df_train_f.drop('precio', axis=1), features))\n",
    "s_test = stack.transform(utils.filtrar_features(df_test_f, features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediccion con todos los features + stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_s = df_train_f.copy()\n",
    "df_test_s = df_test_f.copy()\n",
    "\n",
    "df_train_s['stack01'], df_train_s['stack02'], df_train_s['stack03'], df_train_s['stack04'] = zip(*s_train)\n",
    "df_test_s['stack01'], df_test_s['stack02'], df_test_s['stack03'], df_test_s['stack04'] = zip(*s_test)\n",
    "\n",
    "features_stacking = ['stack01', 'stack02', 'stack03', 'stack04']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_s['id'] = df_train['id']\n",
    "df_test_s['id'] = df_test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/Documents/FIUBA/Datos/.venv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LightGBMWrapper(bagging_fraction=0.8999882607358867, bagging_freq=95,\n",
       "                boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "                feature_fraction=0.2570109385381975, importance_type='split',\n",
       "                learning_rate=0.13601832720254403, max_depth=26, metric='mae',\n",
       "                min_child_samples=20, min_child_weight=0.001,\n",
       "                min_split_gain=0.0, n_estimators=100, n_jobs=-1,\n",
       "                num_boost_round=1500, num_leaves=175, objective='regression',\n",
       "                random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "                subsample=1.0, subsample_for_bin=200000, subsample_freq=0,\n",
       "                test_size=0.08363501292068126)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_2nd = {'bagging_fraction': 0.8999882607358867,\n",
    " 'bagging_freq': int(95.0),\n",
    " 'feature_fraction': 0.2570109385381975,\n",
    " 'learning_rate': 0.13601832720254403,\n",
    " 'max_depth': int(26.0),\n",
    " 'num_leaves': int(175.0),\n",
    " 'test_size': 0.08363501292068126,\n",
    " 'boosting_type': 'dart',\n",
    " 'num_boost_round': 1500,\n",
    " 'objective': 'regression',\n",
    " 'metric': 'mae'}\n",
    "\n",
    "lgb_m_2nd = LightGBMWrapper(**params_2nd)\n",
    "lgb_m_2nd.fit(utils.filtrar_features(df_train_s, features + features_stacking), df_train['precio'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Stacking-full: 514117.80862808606\n"
     ]
    }
   ],
   "source": [
    "df_test_s['target'] = lgb_m_2nd.predict(utils.filtrar_features(df_test_s, features + features_stacking))\n",
    "df_test_s[['id', 'target']].to_csv('respuesta41.csv', index = False)\n",
    "\n",
    "print(f'MAE Stacking-full: {utils.MAE(df_test_s[\"precio\"].values, df_test_s[\"target\"].values)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediccion solo con features de stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_2nd = {'bagging_fraction': 0.8924398062087346,\n",
    "#  'bagging_freq': int(36.0),\n",
    "#  'feature_fraction': 0.16167385124183287,\n",
    "#  'learning_rate': 0.054693418899570134,\n",
    "#  'max_depth': int(4.0),\n",
    "#  'num_leaves': int(93.0),\n",
    "#  'objective': 'regression',\n",
    "#  'boosting_type': 'gbdt',\n",
    "#  'metric': 'mae'}\n",
    "\n",
    "params_2nd = {'bagging_fraction': 0.8243831977099841,\n",
    " 'bagging_freq': int(10.0),\n",
    " 'feature_fraction': 0.9228324501365147,\n",
    " 'learning_rate': 0.050664243951241736,\n",
    " 'max_depth': int(3.0),\n",
    " 'num_leaves': int(78.0),\n",
    " 'objective': 'regression',\n",
    " 'boosting_type': 'dart',\n",
    " 'metric': 'mae'}\n",
    "\n",
    "\n",
    "lgb_m_2nd = LightGBMWrapper(**params_2nd)\n",
    "lgb_m_2nd.fit(s_train, df_train_f['precio'].values)\n",
    "\n",
    "df_test_f['target'] = lgb_m_2nd.predict(s_test)\n",
    "df_test_f[['id', 'target']].to_csv('respuesta43.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [17:21<00:00,  2.60s/it, best loss: 511432.09539443516]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.8243831977099841,\n",
       " 'bagging_freq': 10.0,\n",
       " 'feature_fraction': 0.9228324501365147,\n",
       " 'learning_rate': 0.050664243951241736,\n",
       " 'max_depth': 3.0,\n",
       " 'num_leaves': 78.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = ['stack01', 'stack02', 'stack03', 'stack04']\n",
    "\n",
    "def eval_lightgbm(args):\n",
    "    num_leaves, learning_rate, feature_fraction, bagging_fraction, bagging_freq, max_depth = args\n",
    "\n",
    "    lgb_train = lgb.Dataset(s_train, df_train['precio'].values)\n",
    "    \n",
    "    num_leaves = int(num_leaves)\n",
    "    bagging_freq = int(bagging_freq)\n",
    "    max_depth = int(max_depth)\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'metric': {'mae'}, # Si se deja vacio se toma el ideal para llegar al 'objective'\n",
    "        'num_leaves': num_leaves,\n",
    "        'learning_rate': learning_rate,\n",
    "        'feature_fraction': feature_fraction,\n",
    "        'bagging_fraction': bagging_fraction,\n",
    "        'bagging_freq': bagging_freq,\n",
    "        'max_depth': max_depth,\n",
    "        'verbose': -1,\n",
    "    }\n",
    "\n",
    "    gbm = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=250,\n",
    "                    verbose_eval=-1)\n",
    "    \n",
    "    y_pred_test = gbm.predict(s_test, num_iteration=gbm.best_iteration)\n",
    "    return utils.MAE(df_test['precio'].values, y_pred_test)\n",
    "\n",
    "space = [hp.quniform('num_leaves', 15, 130, 1), hp.uniform('learning_rate', 0.05, 0.9),\n",
    "        hp.uniform('feature_fraction', 0.90, 1), hp.uniform('bagging_fraction', 0.70, 1),\n",
    "        hp.quniform('bagging_freq', 0, 40, 1), hp.quniform('max_depth', 3, 15, 1)]\n",
    "\n",
    "hps = fmin(eval_lightgbm, space=space, algo=tpe.suggest, max_evals=400, verbose=1)\n",
    "\n",
    "display(hps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediccion con promedios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_f['target'] = np.average(s_test, axis=1)\n",
    "df_test_f[['id', 'target']].to_csv('respuesta42.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Stacking only: 519840.412728738\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "y_pred_test = mode(s_test, axis=1)[0]\n",
    "print(f\"MAE Stacking only: {utils.MAE(y_pred_test, df_test_f['precio'].values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Stacking only: 502391.48180612386\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_pred_test = np.average(s_test, axis=1)\n",
    "print(f\"MAE Stacking only: {utils.MAE(y_pred_test, df_test_f['precio'].values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 497375.416620218\n",
       " hess_inv: array([[ 7.64527106e-05, -6.84363088e-05,  5.18102889e-04,\n",
       "         1.30966978e-03],\n",
       "       [-6.84363088e-05,  6.84021400e-05, -4.91721763e-04,\n",
       "        -1.17091868e-03],\n",
       "       [ 5.18102889e-04, -4.91721763e-04,  4.00738244e-03,\n",
       "         9.31303274e-03],\n",
       "       [ 1.30966978e-03, -1.17091868e-03,  9.31303274e-03,\n",
       "         2.30019160e-02]])\n",
       "      jac: array([5.8359375 , 2.96484375, 0.3671875 , 1.5859375 ])\n",
       "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
       "     nfev: 702\n",
       "      nit: 15\n",
       "     njev: 115\n",
       "   status: 2\n",
       "  success: False\n",
       "        x: array([ 0.40410989, -0.16217251,  2.30245548,  0.39816892])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def mae_res(weights):\n",
    "    y_pred_test = np.average(s_test, weights=weights, axis=1)\n",
    "    return utils.MAE(y_pred_test, df_test_f['precio'].values)\n",
    "\n",
    "x0 = [1] * len(s_test.T)\n",
    "minimize(mae_res, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 497375.4169548869\n",
       "     jac: array([-0.58207661, -0.83819032, -0.23283064,  1.65309757])\n",
       " message: 'Optimization terminated successfully.'\n",
       "    nfev: 425\n",
       "     nit: 5\n",
       " success: True\n",
       "       x: array([ 0.49248228, -0.19790651,  2.80637364,  0.48548681])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize, differential_evolution\n",
    "\n",
    "def mae_res(weights):\n",
    "    y_pred_test = np.average(s_test, weights=weights, axis=1)\n",
    "    return utils.MAE(y_pred_test, df_test_f['precio'].values)\n",
    "\n",
    "x0 = [(-3, 4)] * len(s_test.T)\n",
    "differential_evolution(mae_res, bounds=x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
