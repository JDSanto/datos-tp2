{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp\n",
    "import ipynb.fs.full.utils as utils\n",
    "import ipynb.fs.full.features as features\n",
    "import ipynb.fs.full.features_distancias as f_distancias\n",
    "\n",
    "df_train = pd.read_csv('./data/train_filtrado.csv')\n",
    "# Para usarse con el submit a Kaggle\n",
    "df_test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "df_train, df_test = features.features_de_csvs(df_train, df_test)\n",
    "\n",
    "# Randoms solo para asegurarse que los features esten bien hechos\n",
    "# df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "# df_train = utils.dolarizar_df(df_train)\n",
    "# df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df_train, df_test = utils.dividir_df_testeo(df_train, test_size=0.15)\n",
    "\n",
    "# df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "# df_test = utils.pesificar_df(df_test)\n",
    "# df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "df_test = features.llenar_nulls(df_test, hgb_mean=True, df_fill=df_train)\n",
    "df_train = features.llenar_nulls(df_train, hgb_mean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_f = features.features_independientes_precio(df_test)\n",
    "df_test_f = features.features_dependientes_precio(df_test_f, df_train)\n",
    "\n",
    "df_train_f = features.features_independientes_precio(df_train)\n",
    "df_train_f = features.features_dependientes_precio(df_train_f, df_train)\n",
    "\n",
    "df_test_f, cols_tipodepropiedad_ohe = features.columna_a_ohe(df_test_f, 'tipodepropiedad', N=100, df_aux=df_train, devolver_cols=True)\n",
    "df_test_f, cols_provincia_ohe = features.columna_a_ohe(df_test_f, 'provincia', N=100, df_aux=df_train, devolver_cols=True)\n",
    "df_test_f, cols_zona_ohe = features.columna_a_ohe(df_test_f, 'zona', df_aux=df_train_f, devolver_cols=True)\n",
    "\n",
    "df_train_f = features.columna_a_ohe(df_train_f, 'tipodepropiedad', N=100, df_aux=df_test)\n",
    "df_train_f = features.columna_a_ohe(df_train_f, 'provincia', N=100, df_aux=df_test)\n",
    "df_train_f = features.columna_a_ohe(df_train_f, 'zona', df_aux=df_test_f)\n",
    "\n",
    "\n",
    "df_train_f['fecha'] = pd.to_datetime(df_train_f['fecha']).astype(int)\n",
    "df_test_f['fecha'] = pd.to_datetime(df_test_f['fecha']).astype(int)\n",
    "\n",
    "df_train_idf = pd.read_csv('./data/train_idf.csv')\n",
    "df_test_idf = pd.read_csv('./data/test_idf.csv')\n",
    "\n",
    "df_train_f = pd.merge(df_train_f, df_train_idf, on= 'id', how= 'left')\n",
    "df_test_f = pd.merge(df_test_f, df_test_idf, on= 'id', how= 'left')\n",
    "\n",
    "df_train_f = f_distancias.feature_distancias(df_train_f)\n",
    "df_test_f = f_distancias.feature_distancias(df_test_f, df_train_f)\n",
    "\n",
    "\n",
    "# df_train_f = features.KD_feature(df_train_f)\n",
    "# df_test_f =  features.KD_feature(df_test_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "200656/200656 [==============================] - 19s 95us/step - loss: 711513.4986 - mean_squared_error: 1457718427648.0000\n",
      "Epoch 2/15\n",
      "200656/200656 [==============================] - 17s 82us/step - loss: 629041.0194 - mean_squared_error: 1096927412224.0000\n",
      "Epoch 3/15\n",
      "200656/200656 [==============================] - 13s 66us/step - loss: 620967.3106 - mean_squared_error: 1073275535360.0000\n",
      "Epoch 4/15\n",
      "200656/200656 [==============================] - 11s 56us/step - loss: 614176.7622 - mean_squared_error: 1054198595584.0000\n",
      "Epoch 5/15\n",
      "200656/200656 [==============================] - 12s 62us/step - loss: 610162.0927 - mean_squared_error: 1041115709440.0000\n",
      "Epoch 6/15\n",
      "200656/200656 [==============================] - 13s 62us/step - loss: 606702.1105 - mean_squared_error: 1029111218176.0000\n",
      "Epoch 7/15\n",
      "200656/200656 [==============================] - 12s 62us/step - loss: 603282.7526 - mean_squared_error: 1022749048832.0000\n",
      "Epoch 8/15\n",
      "200656/200656 [==============================] - 12s 62us/step - loss: 601646.0045 - mean_squared_error: 1015979900928.0000\n",
      "Epoch 9/15\n",
      "200656/200656 [==============================] - 13s 62us/step - loss: 600207.3626 - mean_squared_error: 1017999523840.0000\n",
      "Epoch 10/15\n",
      "200656/200656 [==============================] - 11s 55us/step - loss: 597731.9743 - mean_squared_error: 1009463984128.0000\n",
      "Epoch 11/15\n",
      "200656/200656 [==============================] - 20s 98us/step - loss: 599013.8639 - mean_squared_error: 1010268176384.0000\n",
      "Epoch 12/15\n",
      "200656/200656 [==============================] - 30s 147us/step - loss: 597375.6887 - mean_squared_error: 1010817892352.0000\n",
      "Epoch 13/15\n",
      "200656/200656 [==============================] - 29s 147us/step - loss: 597543.6022 - mean_squared_error: 1005879754752.0000\n",
      "Epoch 14/15\n",
      "200656/200656 [==============================] - 29s 143us/step - loss: 596810.3896 - mean_squared_error: 1003302289408.0000\n",
      "Epoch 15/15\n",
      "200656/200656 [==============================] - 30s 152us/step - loss: 595548.2007 - mean_squared_error: 993587232768.0000\n",
      "MAE Keras (train): 537271.61629\n",
      "MAE Keras (test): 559996.65594\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, AlphaDropout\n",
    "\n",
    "def normalizar_df(df, features):\n",
    "    min_max = StandardScaler()\n",
    "    df[features] = pd.DataFrame(min_max.fit_transform(df[features]), columns=features)\n",
    "    return df\n",
    "    \n",
    "\n",
    "\n",
    "features = ['habitaciones', 'garages','banos','antiguedad', 'metroscubiertos',  'metrostotales','lat_norm', \n",
    "           'lng_norm', 'gimnasio', 'usosmultiples', 'piscina']\n",
    "\n",
    "features_test = ['top_provincia', 'promedio_precio_ciudad', 'anio', 'promedio_id_zona', \n",
    "                 'promedio_precio_tipo_propiedad', 'count_id_zona', 'count_ciudad', 'puntaje', \n",
    "               'count_tipo_propiedad_ciudad', 'promedio_precio_tipo_propiedad_ciudad_gen','count_id_zona',\n",
    "           'dias_desde_datos','meses_desde_datos','porcentaje_metros','distancia_ciudad_centrica', 'puntaje', \n",
    "                 'distancia_centro_mexico', 'promedio_precio_hbg_tipo_propiedad_provincia']\n",
    "\n",
    "\n",
    "features_cat = ['provincia', 'tipodepropiedad', 'intervalo_metros_totales', 'intervalo_metros_cubiertos',\n",
    "               'zona']\n",
    "\n",
    "features += features_test + features_cat\n",
    "\n",
    "df_train_g = utils.filtrar_features(df_train_f, features, 'precio')\n",
    "\n",
    "features_a_normalizar = [f for f in features if f not in features_cat + ['precio']]\n",
    "\n",
    "df_train_n = pd.get_dummies(df_train_g, columns=features_cat)\n",
    "df_train_n = normalizar_df(df_train_n, features_a_normalizar)\n",
    "\n",
    "x_train, x_test, y_train, y_test = utils.dividir_dataset(df_train_n, 'precio', test_size=0.001)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=250, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(units=250, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(units=250, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(units=250, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(units=250, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "model.fit(x_train.values, y_train.values, epochs=15)\n",
    "\n",
    "y_pred_train = model.predict(x_train.values)\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "keras_mae_train = utils.MAE(y_train.values, y_pred_train)\n",
    "keras_mae_test = utils.MAE(y_test, y_pred_test)\n",
    "print(f\"MAE Keras (train): {keras_mae_train:.5f}\")\n",
    "print(f\"MAE Keras (test): {keras_mae_test:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp\n",
    "import ipynb.fs.full.utils as utils\n",
    "from ipynb.fs.full.features import features_independientes_precio, features_dependientes_precio, features_de_csvs\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "    \n",
    "def normalizar_df(df, features):\n",
    "    min_max = MinMaxScaler()\n",
    "    df[features] = pd.DataFrame(min_max.fit_transform(df[features]), columns=features)\n",
    "    return df\n",
    "\n",
    "def llenar_nulls(df):\n",
    "    df['metrostotales'] = df['metrostotales'].fillna(df['metroscubiertos'])\n",
    "    df['metroscubiertos'] = df['metrostotales'].fillna(df['metrostotales'])\n",
    "    \n",
    "    df['habitaciones'] = df['habitaciones'].fillna(df['habitaciones'].mean())\n",
    "    df['garages'] = df['garages'].fillna(df['garages'].mean())\n",
    "    df['banos'] = df['banos'].fillna(df['banos'].mean())\n",
    "    \n",
    "    df['fecha'] = pd.to_datetime(df['fecha']).astype(int)\n",
    "\n",
    "    \n",
    "df_train = pd.read_csv('./data/train.csv')\n",
    "\n",
    "# Para usarse con el submit a Kaggle\n",
    "df_test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "df_train, df_test = features_de_csvs(df_train, df_test)\n",
    "\n",
    "\n",
    "\n",
    "# df_train = utils.dolarizar_df(df_train)\n",
    "\n",
    "df_test_f = features_independientes_precio(df_test)\n",
    "df_test_f = features_dependientes_precio(df_test_f, df_train)\n",
    "\n",
    "df_train_f = features_independientes_precio(df_train)\n",
    "df_train_f = features_dependientes_precio(df_train_f, df_train)\n",
    "\n",
    "llenar_nulls(df_train_f)\n",
    "llenar_nulls(df_test_f)\n",
    "\n",
    "\n",
    "features = ['habitaciones', 'garages', 'banos', 'fecha',\n",
    "       'metroscubiertos', 'metrostotales', 'provincia',\n",
    "       'gimnasio', 'usosmultiples', 'piscina', 'escuelascercanas', 'centroscomercialescercanos']\n",
    "\n",
    "features_test = ['similares_count',\n",
    "       'porcentaje_metros', 'diferencia_metros', 'intervalo_metros_totales',\n",
    "       'intervalo_metros_cubiertos', 'escomercial',\n",
    "       'promedio_metros_tipo_propiedad', 'promedio_metros_cub_tipo_propiedad',\n",
    "       'tipo_propiedad_compartida', 'prop_frecuente', 'zona', 'top_provincia',\n",
    "       'es_ciudad_centrica', 'promedio_metros_totales_provincia',\n",
    "       'promedio_metros_cubiertos_provincia', 'anio', 'mes', 'dia',\n",
    "       'trimestre', 'escualas_centros_cercanos', 'delincuencia', 'turismo',\n",
    "       'es_antigua', 'cantidad_inquilinos', 'metros_totales_normalizados',\n",
    "       'metros_cubiertos_normalizados', 'promedio_precio_provincia',\n",
    "       'promedio_precio_ciudad', 'promedio_precio_ciudad_gen',\n",
    "       'varianza_precio_ciudad', 'count_ciudad', 'promedio_id_zona',\n",
    "       'promedio_id_zona_gen', 'varianza_id_zona', 'count_id_zona',\n",
    "       'promedio_precio_tipo_propiedad',\n",
    "       'promedio_precio_tipo_propiedad_ciudad',\n",
    "       'promedio_precio_tipo_propiedad_ciudad_gen', 'count_tipo_propiedad',\n",
    "       'count_tipo_propiedad_ciudad', 'promedio_por_mes', 'varianza_por_mes',\n",
    "       'promedio_precio_habitaciones',\n",
    "       'promedio_precio_habitaciones_banos_garages',\n",
    "       'promedio_precio_banos_garages', 'promedio_precio_hbg_tipo_propiedad',\n",
    "       'promedio_precio_hbg_tipo_propiedad_provincia',\n",
    "       'promedio_precio_booleanos', 'puntaje']\n",
    "\n",
    "features_cat = ['provincia', 'tipodepropiedad', 'intervalo_metros_totales', 'intervalo_metros_cubiertos',\n",
    "               'zona', 'escualas_centros_cercanos']\n",
    "\n",
    "features += features_test + features_cat\n",
    "\n",
    "\n",
    "features_a_normalizar = [f for f in features if f not in features_cat + ['precio']]\n",
    "\n",
    "df_full = pd.concat([df_test_f, df_train_f.drop('precio', axis=1)], ignore_index=True)\n",
    "df_full_g = utils.filtrar_features(df_full, features, 'precio')\n",
    "\n",
    "df_full_ohe = pd.get_dummies(df_full_g, columns=features_cat)\n",
    "\n",
    "df_train_n = df_full_ohe.iloc[len(df_test_f.values):len(df_full_ohe.values)].reset_index()\n",
    "df_train_n['precio'] = df_train_f['precio']\n",
    "df_test_n = df_full_ohe.iloc[0:len(df_test_f.values)].reset_index()\n",
    "\n",
    "x_train, x_test, y_train, y_test = utils.dividir_dataset(df_train_n, 'precio', test_size=0.05)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['accuracy'], validation_split=0.1)\n",
    "\n",
    "model.fit(x_train.values, y_train.values, epochs=15)\n",
    "\n",
    "y_pred_train = model.predict(x_train)\n",
    "\n",
    "keras_mae_train = utils.MAE(y_train, y_pred_train)\n",
    "print(f\"MAE Keras (train): {keras_mae_train:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def full_display(df):\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalizar_df(df, features, min_max=None):\n",
    "    if min_max is None:\n",
    "        min_max = MinMaxScaler()\n",
    "        df[features] = pd.DataFrame(min_max.fit_transform(df[features]), columns=features)\n",
    "        return df, min_max\n",
    "    else:\n",
    "        df[features] = pd.DataFrame(min_max.transform(df[features]), columns=features)\n",
    "        return df, min_max\n",
    "    \n",
    "\n",
    "def llenar_nulls(df):\n",
    "    df['metrostotales'] = df['metrostotales'].fillna(df['metroscubiertos'])\n",
    "    df['metroscubiertos'] = df['metrostotales'].fillna(df['metrostotales'])\n",
    "    \n",
    "    df['habitaciones'] = df['habitaciones'].fillna(df['habitaciones'].mean())\n",
    "    df['garages'] = df['garages'].fillna(df['garages'].mean())\n",
    "    df['banos'] = df['banos'].fillna(df['banos'].mean())\n",
    "    \n",
    "    df['fecha'] = pd.to_datetime(df['fecha']).astype(int)\n",
    "\n",
    "\n",
    "llenar_nulls(df_train_f)\n",
    "llenar_nulls(df_test_f)\n",
    "\n",
    "\n",
    "features = ['habitaciones', 'garages', 'banos', 'fecha',\n",
    "       'metroscubiertos', 'metrostotales', 'provincia',\n",
    "       'gimnasio', 'usosmultiples', 'piscina', 'escuelascercanas', 'centroscomercialescercanos']\n",
    "\n",
    "features_test = ['similares_count',\n",
    "       'porcentaje_metros', 'diferencia_metros', 'intervalo_metros_totales',\n",
    "       'intervalo_metros_cubiertos', 'escomercial',\n",
    "       'promedio_metros_tipo_propiedad', 'promedio_metros_cub_tipo_propiedad',\n",
    "       'tipo_propiedad_compartida', 'prop_frecuente', 'zona', 'top_provincia',\n",
    "       'es_ciudad_centrica', 'promedio_metros_totales_provincia',\n",
    "       'promedio_metros_cubiertos_provincia', 'anio', 'mes', 'dia',\n",
    "       'trimestre', 'escualas_centros_cercanos', 'delincuencia', 'turismo',\n",
    "       'es_antigua', 'cantidad_inquilinos', 'metros_totales_normalizados',\n",
    "       'metros_cubiertos_normalizados', 'promedio_precio_provincia',\n",
    "       'promedio_precio_ciudad', 'promedio_precio_ciudad_gen',\n",
    "       'varianza_precio_ciudad', 'count_ciudad', 'promedio_id_zona',\n",
    "       'promedio_id_zona_gen', 'varianza_id_zona', 'count_id_zona',\n",
    "       'promedio_precio_tipo_propiedad',\n",
    "       'promedio_precio_tipo_propiedad_ciudad',\n",
    "       'promedio_precio_tipo_propiedad_ciudad_gen', 'count_tipo_propiedad',\n",
    "       'count_tipo_propiedad_ciudad', 'promedio_por_mes', 'varianza_por_mes',\n",
    "       'promedio_precio_habitaciones',\n",
    "       'promedio_precio_habitaciones_banos_garages',\n",
    "       'promedio_precio_banos_garages', 'promedio_precio_hbg_tipo_propiedad',\n",
    "       'promedio_precio_hbg_tipo_propiedad_provincia',\n",
    "       'promedio_precio_booleanos', 'puntaje']\n",
    "\n",
    "features_cat = ['provincia', 'tipodepropiedad', 'intervalo_metros_totales', 'intervalo_metros_cubiertos',\n",
    "               'zona', 'escualas_centros_cercanos']\n",
    "\n",
    "features += features_test + features_cat\n",
    "\n",
    "\n",
    "features_a_normalizar = [f for f in features if f not in features_cat + ['precio']]\n",
    "\n",
    "df_train_g = utils.filtrar_features(df_train_f, features, 'precio')\n",
    "df_train_n = pd.get_dummies(df_train_g, columns=features_cat)\n",
    "df_train_n, min_max = normalizar_df(df_train_n, features_a_normalizar)\n",
    "\n",
    "df_test_z = pd.concat([df_test_f, df_train_f.drop('precio', axis=1)], ignore_index=True, sort=False)\n",
    "df_test_g = utils.filtrar_features(df_test_z, features)\n",
    "df_test_n = pd.get_dummies(df_test_g, columns=features_cat).iloc[0:60000]\n",
    "df_test_n, min_max = normalizar_df(df_test_n, features_a_normalizar, min_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def full_display(df):\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "        display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_display(df_test_n.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_eval = model.predict(df_test_n)\n",
    "df_test_f['target'] = y_pred_eval\n",
    "\n",
    "# df_test_f = utils.pesificar_df(df_test_f, 'target', 'target')\n",
    "df_test_f[['id', 'target']].to_csv('respuesta2-keras.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  model = tf.keras.models.Sequential()\n",
    "  model.add(tf.keras.layers.Dense(10, input_shape=(4,) , activation = 'relu'))\n",
    "  model.add(tf.keras.layers.Dense(10, activation = 'relu'))\n",
    "  model.add(tf.keras.layers.Dense(3, activation = 'softmax'))\n",
    " \n",
    "  model.compile(loss = 'sparse_categorical_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'] )\n",
    " \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91\n",
      "  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
      " 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127\n",
      " 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145\n",
      " 146 147] TEST: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-05180306d6cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TRAIN:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TEST:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    " \n",
    "kf = KFold(n_splits=2)    \n",
    "KFold(n_splits=2, random_state=None, shuffle=False)\n",
    "\n",
    "X = list(df_train_f)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = df_test_f[train_index], df_test[test_index]\n",
    "    \n",
    "    \n",
    "#   model=create_model()\n",
    "#   model.fit(x_train, y_train,epochs=20)\n",
    "  \n",
    "#   print('Model evaluation ',model.evaluate(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-06b54b0be799>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'split' is not defined"
     ]
    }
   ],
   "source": [
    "KFold(n_split).split(df_train_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
