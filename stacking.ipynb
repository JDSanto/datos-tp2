{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp\n",
    "import ipynb.fs.full.utils as utils\n",
    "import ipynb.fs.full.features as features\n",
    "import ipynb.fs.full.features_distancias as f_distancias\n",
    "\n",
    "df_train = pd.read_csv('./data/train_filtrado.csv')\n",
    "# df_train = utils.dolarizar_df(df_train)\n",
    "# Para usarse con el submit a Kaggle\n",
    "df_test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "df_train = features.llenar_nulls(df_train)\n",
    "df_test = features.llenar_nulls(df_test, hgb_mean=True, df_fill=df_train)\n",
    "\n",
    "df_train_cluster = pd.read_csv('./data/clustering_train.csv').rename(columns={'label': 'clustering_label'})\n",
    "df_test_cluster = pd.read_csv('./data/clustering_test.csv').rename(columns={'label': 'clustering_label'})\n",
    "\n",
    "df_train = pd.merge(df_train, df_train_cluster, on='id')\n",
    "df_test = pd.merge(df_test, df_test_cluster, on='id')\n",
    "\n",
    "df_train_idf = pd.read_csv('./data/train_idf.csv')\n",
    "df_test_idf = pd.read_csv('./data/test_idf.csv')\n",
    "\n",
    "df_train = pd.merge(df_train, df_train_idf, on= 'id', how= 'left')\n",
    "df_test = pd.merge(df_test, df_test_idf, on= 'id', how= 'left')\n",
    "\n",
    "df_train, df_test = utils.dividir_df_testeo(df_train, test_size=0.3)\n",
    "\n",
    "# df_train, df_test = features_de_csvs(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train, df_test = utils.dividir_df_testeo(df_train, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_f = features.features_independientes_precio(df_test)\n",
    "df_test_f = features.features_dependientes_precio(df_test_f, df_train)\n",
    "\n",
    "df_train_f = features.features_independientes_precio(df_train)\n",
    "df_train_f = features.features_dependientes_precio(df_train_f, df_train)\n",
    "\n",
    "df_test_f, cols_tipodepropiedad_ohe = features.columna_a_ohe(df_test_f, 'tipodepropiedad', N=100, df_aux=df_train, devolver_cols=True)\n",
    "df_test_f, cols_provincia_ohe = features.columna_a_ohe(df_test_f, 'provincia', N=100, df_aux=df_train, devolver_cols=True)\n",
    "df_test_f, cols_zona_ohe = features.columna_a_ohe(df_test_f, 'zona', df_aux=df_train_f, devolver_cols=True)\n",
    "\n",
    "df_train_f = features.columna_a_ohe(df_train_f, 'tipodepropiedad', N=100, df_aux=df_test)\n",
    "df_train_f = features.columna_a_ohe(df_train_f, 'provincia', N=100, df_aux=df_test)\n",
    "df_train_f = features.columna_a_ohe(df_train_f, 'zona', df_aux=df_test_f)\n",
    "\n",
    "\n",
    "df_train_f['fecha'] = pd.to_datetime(df_train_f['fecha']).astype(int)\n",
    "df_test_f['fecha'] = pd.to_datetime(df_test_f['fecha']).astype(int)\n",
    "\n",
    "\n",
    "df_train_f = f_distancias.feature_distancias(df_train_f)\n",
    "df_test_f = f_distancias.feature_distancias(df_test_f, df_train_f)\n",
    "\n",
    "\n",
    "# df_train_f = features.KD_feature(df_train_f)\n",
    "# df_test_f =  features.KD_feature(df_test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selector de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class FeatureSelector(BaseEstimator):\n",
    "    \n",
    "    base_features = list(df_train_f.drop('precio', axis=1, errors='ignore').columns)\n",
    "    \n",
    "    def __init__(self, features):\n",
    "        base_features = FeatureSelector.base_features\n",
    "\n",
    "        self.features = features\n",
    "        self.features_index = [i for i in range(len(base_features)) if base_features[i] in features]\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        return x[:, self.features_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class LightGBMWrapper(lgb.LGBMRegressor):\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        return super(LightGBMWrapper, self).fit(x, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return super(LightGBMWrapper, self).predict(X)\n",
    "\n",
    "params = {'boosting_type': 'gbdt',\n",
    " 'feature_fraction': 0.8,\n",
    " 'learning_rate': 0.25,\n",
    " 'max_bin': 255,\n",
    " 'max_depth': 15,\n",
    " 'metric': 'mae',\n",
    " 'min_data_in_leaf': 40,\n",
    " 'min_split_gain': 0.7,\n",
    " 'n_jobs': 4,\n",
    " 'num_leaves': 300,\n",
    " 'objective': 'regression',\n",
    " 'reg_lambda': 10,\n",
    " 'verbose': 0}\n",
    "\n",
    "lgb_m = Pipeline(steps=[\n",
    "    ('feature_selector', FeatureSelector(['antiguedad', 'habitaciones', 'garages', 'banos', 'metroscubiertos',\n",
    "       'metrostotales', 'lat', 'lng', 'fecha', 'piscina',\n",
    "       'clustering_label', 'idf_descripcion',\n",
    "       'porcentaje_metros', 'diferencia_metros', 'promedio_metros_tipo_propiedad', \n",
    "       'prop_frecuente', 'top_provincia', 'es_ciudad_centrica',\n",
    "       'mes', 'trimestre', 'dias_desde_datos',\n",
    "       'tam_ambientes', 'promedio_precio_provincia',\n",
    "       'promedio_precio_ciudad', 'count_ciudad',\n",
    "       'promedio_id_zona', 'count_id_zona', 'promedio_precio_tipo_propiedad',\n",
    "       'promedio_precio_tipo_propiedad_ciudad', 'count_tipo_propiedad',\n",
    "       'count_tipo_propiedad_ciudad', 'promedio_por_mes',\n",
    "       'promedio_precio_habitaciones_banos_garages',\n",
    "       'promedio_precio_hbg_tipo_propiedad', 'puntaje',\n",
    "       'distancia_ciudad_centrica', 'distancia_centro_mexico'])),\n",
    "    ('lightgbm', LightGBMWrapper(**params))\n",
    "])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "features = ['antiguedad', 'habitaciones', 'garages', 'banos', 'metroscubiertos', 'metrostotales', 'lat',\n",
    " 'lng', 'fecha', 'gimnasio', 'usosmultiples', 'piscina', 'escuelascercanas', 'centroscomercialescercanos',\n",
    " 'precio', 'clustering_label', 'idf_titulo', 'idf_descripcion', 'peso_titulo', 'peso_descripcion', 'porcentaje_metros', 'diferencia_metros', \n",
    " 'metroscubiertos_bins_unif', 'metroscubiertos_bins_perc', 'metros_totales_normalizados', 'metros_cubiertos_normalizados', \n",
    " 'escomercial', 'promedio_metros_tipo_propiedad', 'promedio_metros_cub_tipo_propiedad', 'tipo_propiedad_compartida',\n",
    " 'prop_frecuente', 'top_provincia', 'es_ciudad_centrica', 'promedio_metros_totales_provincia',\n",
    " 'promedio_metros_cubiertos_provincia', 'anio', 'mes', 'dia', 'trimestre', 'dias_desde_datos',  'meses_desde_datos',  \n",
    " 'delincuencia',  'turismo',  'es_antigua', 'antiguedad_bins_unif', 'antiguedad_bins_perc', \n",
    " 'cantidad_inquilinos',  'tam_ambientes', 'promedio_precio_provincia', 'promedio_precio_ciudad', 'promedio_precio_ciudad_gen', \n",
    " 'varianza_precio_ciudad', 'count_ciudad', 'promedio_id_zona', 'promedio_id_zona_gen',\n",
    " 'varianza_id_zona',  'count_id_zona',  'promedio_precio_tipo_propiedad', 'promedio_precio_tipo_propiedad_ciudad',\n",
    " 'promedio_precio_tipo_propiedad_ciudad_gen', 'count_tipo_propiedad', 'count_tipo_propiedad_ciudad', 'promedio_por_mes', \n",
    " 'varianza_por_mes', 'promedio_precio_habitaciones', 'promedio_precio_habitaciones_banos_garages', 'promedio_precio_banos_garages', \n",
    " 'promedio_precio_hbg_tipo_propiedad', 'lat_norm', 'lng_norm', 'promedio_precio_booleanos', 'puntaje', 'distancia_ciudad_centrica',\n",
    " 'distancia_centro_mexico', 'distancia_ciudad_cara']\n",
    "\n",
    "x_train, x_test, y_train, y_test = utils.dividir_dataset(df_train_f, 'precio', features, test_size=2)\n",
    "\n",
    "lgb_m = LightGBMWrapper(**params)\n",
    "\n",
    "selector = RFECV(lgb_m, cv=4)\n",
    "selector.fit(x_train, y_train)\n",
    "\n",
    "x_train.columns[selector.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "\n",
    "def keras_modelo():    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=200, activation='selu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(units=200, activation='selu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(units=200, activation='selu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_squared_error'])  \n",
    "    return model\n",
    "\n",
    "keras_m = KerasRegressor(build_fn=keras_modelo, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "class XGBoostWrapper(xgb.XGBRegressor):\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        return super(xgb.XGBRegressor, self).fit(x, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return super(xgb.XGBRegressor, self).predict(X)\n",
    "\n",
    "hps = {'verbosity': 0,\n",
    "'subsample': 0.9,\n",
    "'scale_pos_weight': 2,\n",
    "'reg_alpha': 4,\n",
    "'objective': 'reg:squarederror',\n",
    "'n_jobs': 4,\n",
    "'n_estimators': 75,\n",
    "'max_depth': 15,\n",
    "'learning_rate': 0.1,\n",
    "'eval_metric': 'mae',\n",
    "'colsample_bytree': 0.7}\n",
    "\n",
    "xgb_m = Pipeline([\n",
    "    ('feature_selector', FeatureSelector(['antiguedad', 'habitaciones', 'garages', 'banos', 'metroscubiertos',\n",
    "       'metrostotales', 'lat', 'lng', 'gimnasio', 'usosmultiples',\n",
    "       'piscina', 'clustering_label', 'idf_descripcion', 'porcentaje_metros', 'diferencia_metros',\n",
    "       'promedio_metros_tipo_propiedad', 'prop_frecuente', 'top_provincia',\n",
    "       'promedio_metros_totales_provincia', 'anio', 'mes',\n",
    "       'trimestre', 'dias_desde_datos', 'cantidad_inquilinos', 'tam_ambientes', 'promedio_precio_provincia',\n",
    "       'promedio_precio_ciudad', 'count_ciudad', 'promedio_id_zona',\n",
    "       'count_id_zona','promedio_precio_tipo_propiedad',\n",
    "       'promedio_precio_tipo_propiedad_ciudad', 'count_tipo_propiedad',\n",
    "       'count_tipo_propiedad_ciudad', 'promedio_por_mes', \n",
    "       'promedio_precio_banos_garages', 'promedio_precio_hbg_tipo_propiedad',\n",
    "       'promedio_precio_booleanos', 'puntaje',\n",
    "       'distancia_ciudad_centrica', 'distancia_centro_mexico'])),\n",
    "    ('xgboost', XGBoostWrapper(**hps))\n",
    "])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "features = ['antiguedad', 'habitaciones', 'garages', 'banos', 'metroscubiertos', 'metrostotales', 'lat',\n",
    " 'lng', 'fecha', 'gimnasio', 'usosmultiples', 'piscina', 'escuelascercanas', 'centroscomercialescercanos',\n",
    " 'precio', 'clustering_label', 'idf_titulo', 'idf_descripcion', 'peso_titulo', 'peso_descripcion', 'porcentaje_metros', 'diferencia_metros', \n",
    " 'metroscubiertos_bins_unif', 'metroscubiertos_bins_perc', 'metros_totales_normalizados', 'metros_cubiertos_normalizados', \n",
    " 'escomercial', 'promedio_metros_tipo_propiedad', 'promedio_metros_cub_tipo_propiedad', 'tipo_propiedad_compartida',\n",
    " 'prop_frecuente', 'top_provincia', 'es_ciudad_centrica', 'promedio_metros_totales_provincia',\n",
    " 'promedio_metros_cubiertos_provincia', 'anio', 'mes', 'dia', 'trimestre', 'dias_desde_datos',  'meses_desde_datos',  \n",
    " 'delincuencia',  'turismo',  'es_antigua', 'antiguedad_bins_unif', 'antiguedad_bins_perc', \n",
    " 'cantidad_inquilinos',  'tam_ambientes', 'promedio_precio_provincia', 'promedio_precio_ciudad', 'promedio_precio_ciudad_gen', \n",
    " 'varianza_precio_ciudad', 'count_ciudad', 'promedio_id_zona', 'promedio_id_zona_gen',\n",
    " 'varianza_id_zona',  'count_id_zona',  'promedio_precio_tipo_propiedad', 'promedio_precio_tipo_propiedad_ciudad',\n",
    " 'promedio_precio_tipo_propiedad_ciudad_gen', 'count_tipo_propiedad', 'count_tipo_propiedad_ciudad', 'promedio_por_mes', \n",
    " 'varianza_por_mes', 'promedio_precio_habitaciones', 'promedio_precio_habitaciones_banos_garages', 'promedio_precio_banos_garages', \n",
    " 'promedio_precio_hbg_tipo_propiedad', 'lat_norm', 'lng_norm', 'promedio_precio_booleanos', 'puntaje', 'distancia_ciudad_centrica',\n",
    " 'distancia_centro_mexico', 'distancia_ciudad_cara']\n",
    "\n",
    "x_train, x_test, y_train, y_test = utils.dividir_dataset(df_train_f, 'precio', features, test_size=2)\n",
    "\n",
    "hps['n_estimators'] = 20\n",
    "\n",
    "xgb_m_rfecv = XGBoostWrapper(**hps)\n",
    "\n",
    "selector = RFECV(xgb_m_rfecv, cv=4)\n",
    "selector.fit(x_train, y_train)\n",
    "\n",
    "x_train.columns[selector.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "params = {'bootstrap': False,\n",
    "          'max_features': 'sqrt',\n",
    "          'min_samples_split': 4,\n",
    "          'n_jobs': 2,\n",
    "          'n_estimators': 100}\n",
    "\n",
    "forest_m = Pipeline([\n",
    "    ('feature_selector', FeatureSelector(['antiguedad', 'garages', 'banos', 'metroscubiertos', 'metrostotales',\n",
    "       'lat', 'lng', 'fecha', 'idf_descripcion', 'porcentaje_metros', 'diferencia_metros',\n",
    "       'metroscubiertos_bins_unif', 'metroscubiertos_bins_perc',\n",
    "       'metros_totales_normalizados', 'metros_cubiertos_normalizados',\n",
    "       'promedio_metros_tipo_propiedad',\n",
    "       'promedio_metros_cubiertos_provincia', 'mes', 'dias_desde_datos', 'tam_ambientes',\n",
    "       'promedio_precio_provincia', 'promedio_precio_ciudad', 'count_ciudad',\n",
    "       'promedio_id_zona', 'count_id_zona', 'promedio_precio_tipo_propiedad',\n",
    "       'promedio_precio_tipo_propiedad_ciudad',\n",
    "       'count_tipo_propiedad_ciudad', 'promedio_por_mes', 'promedio_precio_hbg_tipo_propiedad',\n",
    "       'puntaje', 'distancia_ciudad_centrica',\n",
    "       'distancia_centro_mexico'])),\n",
    "    ('random_forest', RandomForestRegressor(**params))\n",
    "])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "features = ['antiguedad', 'habitaciones', 'garages', 'banos', 'metroscubiertos', 'metrostotales', 'lat',\n",
    " 'lng', 'fecha', 'gimnasio', 'usosmultiples', 'piscina', 'escuelascercanas', 'centroscomercialescercanos',\n",
    " 'precio', 'clustering_label', 'idf_titulo', 'idf_descripcion', 'peso_titulo', 'peso_descripcion', 'porcentaje_metros', 'diferencia_metros', \n",
    " 'metroscubiertos_bins_unif', 'metroscubiertos_bins_perc', 'metros_totales_normalizados', 'metros_cubiertos_normalizados', \n",
    " 'escomercial', 'promedio_metros_tipo_propiedad', 'promedio_metros_cub_tipo_propiedad', 'tipo_propiedad_compartida',\n",
    " 'prop_frecuente', 'top_provincia', 'es_ciudad_centrica', 'promedio_metros_totales_provincia',\n",
    " 'promedio_metros_cubiertos_provincia', 'anio', 'mes', 'dia', 'trimestre', 'dias_desde_datos',  'meses_desde_datos',  \n",
    " 'delincuencia',  'turismo',  'es_antigua', 'antiguedad_bins_unif', 'antiguedad_bins_perc', \n",
    " 'cantidad_inquilinos',  'tam_ambientes', 'promedio_precio_provincia', 'promedio_precio_ciudad', 'promedio_precio_ciudad_gen', \n",
    " 'varianza_precio_ciudad', 'count_ciudad', 'promedio_id_zona', 'promedio_id_zona_gen',\n",
    " 'varianza_id_zona',  'count_id_zona',  'promedio_precio_tipo_propiedad', 'promedio_precio_tipo_propiedad_ciudad',\n",
    " 'promedio_precio_tipo_propiedad_ciudad_gen', 'count_tipo_propiedad', 'count_tipo_propiedad_ciudad', 'promedio_por_mes', \n",
    " 'varianza_por_mes', 'promedio_precio_habitaciones', 'promedio_precio_habitaciones_banos_garages', 'promedio_precio_banos_garages', \n",
    " 'promedio_precio_hbg_tipo_propiedad', 'lat_norm', 'lng_norm', 'promedio_precio_booleanos', 'puntaje', 'distancia_ciudad_centrica',\n",
    " 'distancia_centro_mexico', 'distancia_ciudad_cara']\n",
    "\n",
    "x_train, x_test, y_train, y_test = utils.dividir_dataset(df_train_f, 'precio', features, test_size=2)\n",
    "\n",
    "params['n_estimators'] = 20\n",
    "\n",
    "forest_m_rfecv = RandomForestRegressor(**params)\n",
    "\n",
    "selector = RFECV(forest_m_rfecv, cv=4)\n",
    "selector.fit(x_train, y_train)\n",
    "\n",
    "x_train.columns[selector.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "params = {'criterion': 'mse',\n",
    " 'max_features': 'sqrt',\n",
    " 'min_samples_split': 4}\n",
    "\n",
    "extratrees_m =  Pipeline([\n",
    "    ('feature_selector', FeatureSelector(['antiguedad', 'garages', 'banos', 'metroscubiertos', 'metrostotales',\n",
    "       'lat', 'lng', 'fecha', 'idf_titulo', 'idf_descripcion',\n",
    "       'peso_descripcion', 'porcentaje_metros', 'diferencia_metros',\n",
    "       'metroscubiertos_bins_unif', 'metroscubiertos_bins_perc',\n",
    "       'metros_totales_normalizados', 'metros_cubiertos_normalizados',\n",
    "       'promedio_metros_tipo_propiedad', 'promedio_metros_cub_tipo_propiedad',\n",
    "       'promedio_metros_cubiertos_provincia', 'mes', 'dia', 'dias_desde_datos',\n",
    "       'meses_desde_datos', 'antiguedad_bins_perc', 'tam_ambientes',\n",
    "       'promedio_precio_provincia', 'promedio_precio_ciudad',\n",
    "       'promedio_precio_ciudad_gen', 'varianza_precio_ciudad', 'count_ciudad',\n",
    "       'promedio_id_zona', 'promedio_id_zona_gen', 'varianza_id_zona',\n",
    "       'count_id_zona', 'promedio_precio_tipo_propiedad',\n",
    "       'promedio_precio_tipo_propiedad_ciudad',\n",
    "       'promedio_precio_tipo_propiedad_ciudad_gen',\n",
    "       'count_tipo_propiedad_ciudad', 'promedio_por_mes', 'varianza_por_mes',\n",
    "       'promedio_precio_habitaciones_banos_garages',\n",
    "       'promedio_precio_banos_garages', 'promedio_precio_hbg_tipo_propiedad',\n",
    "       'lat_norm', 'lng_norm', 'puntaje', 'distancia_ciudad_centrica',\n",
    "       'distancia_centro_mexico', 'distancia_ciudad_cara'])),\n",
    "    ('extra_trees', ExtraTreesRegressor(n_estimators=50, **params))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "class CatBoostWrapper(CatBoostRegressor):\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        # posiciones de features categoricas encontradas a mano\n",
    "        cat_features = [0, 1, 2]\n",
    "        return super(CatBoostWrapper, self).fit(x, y, cat_features=cat_features)\n",
    "\n",
    "\n",
    "f_s = FeatureSelector(['antiguedad', 'garages', 'banos', 'metroscubiertos', 'metrostotales',\n",
    "       'lat', 'lng', 'fecha', 'idf_descripcion', 'porcentaje_metros', 'diferencia_metros',\n",
    "       'promedio_metros_tipo_propiedad', 'promedio_metros_cubiertos_provincia', 'mes', \n",
    "       'dias_desde_datos', 'tam_ambientes',\n",
    "       'promedio_precio_provincia', 'promedio_precio_ciudad',\n",
    "       'count_ciudad', 'promedio_id_zona',\n",
    "       'count_id_zona', 'promedio_precio_tipo_propiedad',\n",
    "       'promedio_precio_tipo_propiedad_ciudad',\n",
    "       'count_tipo_propiedad_ciudad', 'promedio_por_mes', 'promedio_precio_hbg_tipo_propiedad',\n",
    "       'puntaje', 'distancia_ciudad_centrica',\n",
    "       'distancia_centro_mexico', 'provincia', 'ciudad', 'tipodepropiedad'])\n",
    "\n",
    "# params = {'od_wait': 50,\n",
    "#  'od_type': 'IncToDec',\n",
    "#  'learning_rate': 0.2,\n",
    "#  'l2_leaf_reg': 0,\n",
    "#  'depth': 10,\n",
    "#  'iterations': 300,\n",
    "#  'silent': True,\n",
    "#  'eval_metric': 'MAE',\n",
    "# }\n",
    "\n",
    "params = {\n",
    "    'od_wait': 50,\n",
    "    'od_type': 'Iter',\n",
    "    'learning_rate': 0.15,\n",
    "    'l2_leaf_reg': 3,\n",
    "    'depth': 12,\n",
    "    'colsample_bylevel': 0.5,\n",
    "    'border_count': 128,\n",
    "    'iterations': 300,\n",
    "    'silent': True,\n",
    "    'eval_metric': 'MAE'\n",
    "}\n",
    "\n",
    "\n",
    "catboost_m = Pipeline([\n",
    "    ('feature_selector', f_s),\n",
    "    ('catboost', CatBoostWrapper(**params))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [regression]\n",
      "metric:       [mean_absolute_error]\n",
      "variant:      [A]\n",
      "n_estimators: [5]\n",
      "\n",
      "estimator  0: [lightgbm: Pipeline]\n",
      "    fold  0:  [484282.99385412]\n",
      "    fold  1:  [483709.40052008]\n",
      "    fold  2:  [486961.11769344]\n",
      "    fold  3:  [480640.37270216]\n",
      "    ----\n",
      "    MEAN:     [483898.47119245] + [2246.03065200]\n",
      "\n",
      "estimator  1: [randomforest: Pipeline]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/javier/Documents/FIUBA/Datos/.venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-12-2af6ee0b717e>\", line 17, in <module>\n",
      "    df_train_f['precio'].values)\n",
      "  File \"/home/javier/Documents/FIUBA/Datos/datos-tp2/vecstack_sk.py\", line 506, in fit\n",
      "    transform=self.transform_target)\n",
      "  File \"/home/javier/Documents/FIUBA/Datos/datos-tp2/vecstack_sk.py\", line 825, in _estimator_action\n",
      "    return estimator.fit(X_train, self._transformer(y_train, func=transform))\n",
      "  File \"/home/javier/Documents/FIUBA/Datos/.venv/lib/python3.7/site-packages/sklearn/pipeline.py\", line 356, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params)\n",
      "  File \"/home/javier/Documents/FIUBA/Datos/.venv/lib/python3.7/site-packages/sklearn/ensemble/forest.py\", line 330, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"/home/javier/Documents/FIUBA/Datos/.venv/lib/python3.7/site-packages/joblib/parallel.py\", line 1016, in __call__\n",
      "    self.retrieve()\n",
      "  File \"/home/javier/Documents/FIUBA/Datos/.venv/lib/python3.7/site-packages/joblib/parallel.py\", line 908, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 651, in get\n",
      "    self.wait(timeout)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 648, in wait\n",
      "    self._event.wait(timeout)\n",
      "  File \"/usr/lib/python3.7/threading.py\", line 552, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"/usr/lib/python3.7/threading.py\", line 296, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/javier/Documents/FIUBA/Datos/.venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/javier/Documents/FIUBA/Datos/.venv/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/javier/Documents/FIUBA/Datos/.venv/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/javier/Documents/FIUBA/Datos/.venv/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/usr/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/usr/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/usr/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from vecstack import stacking\n",
    "from vecstack_sk import StackingTransformer\n",
    "\n",
    "modelos = [\n",
    "           ('lightgbm', lgb_m), \n",
    "           ('randomforest', forest_m),\n",
    "           ('extratrees', extratrees_m),\n",
    "           ('catboost', catboost_m), \n",
    "           ('xgboost', xgb_m)\n",
    "          ]\n",
    "\n",
    "stack = StackingTransformer(modelos, \n",
    "                          regression=True, verbose=2, n_folds=4)\n",
    "\n",
    "stack.fit(df_train_f.drop('precio', axis=1).values, \n",
    "                          df_train_f['precio'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set was detected.\n",
      "Transforming...\n",
      "\n",
      "estimator  0: [lightgbm: Pipeline]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  1: [catboost: Pipeline]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  2: [randomforest: Pipeline]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  3: [extratrees: Pipeline]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  4: [xgboost: Pipeline]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "Transforming...\n",
      "\n",
      "estimator  0: [lightgbm: Pipeline]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  1: [catboost: Pipeline]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  2: [randomforest: Pipeline]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  3: [extratrees: Pipeline]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  4: [xgboost: Pipeline]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_train = stack.transform(df_train_f.drop('precio', axis=1).values)\n",
    "s_test = stack.transform(df_test_f.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediccion con todos los features + stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_s = df_train_f.copy()\n",
    "df_test_s = df_test_f.copy()\n",
    "\n",
    "df_train_s['stack01'], df_train_s['stack02'], df_train_s['stack03'], df_train_s['stack04'], df_train_s['stack05'] = zip(*s_train)\n",
    "df_test_s['stack01'], df_test_s['stack02'], df_test_s['stack03'], df_test_s['stack04'], df_test_s['stack05'] = zip(*s_test)\n",
    "\n",
    "features_stacking = ['stack01', 'stack02', 'stack03', 'stack04', 'stack05']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_s['id'] = df_train['id']\n",
    "df_test_s['id'] = df_test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/Documents/FIUBA/Datos/.venv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LightGBMWrapper(bagging_fraction=0.8999882607358867, bagging_freq=95,\n",
       "                boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
       "                feature_fraction=0.2570109385381975, importance_type='split',\n",
       "                learning_rate=0.13601832720254403, max_depth=26, metric='mae',\n",
       "                min_child_samples=20, min_child_weight=0.001,\n",
       "                min_split_gain=0.0, n_estimators=100, n_jobs=-1,\n",
       "                num_boost_round=1200, num_leaves=175, objective='regression',\n",
       "                random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "                subsample=1.0, subsample_for_bin=200000, subsample_freq=0,\n",
       "                test_size=0.08363501292068126)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['habitaciones', 'garages','banos','antiguedad', 'metroscubiertos',  'metrostotales','lat_norm', \n",
    "           'lng_norm', 'gimnasio', 'usosmultiples', 'piscina']\n",
    "\n",
    "features_test = ['prop_frecuente', 'top_provincia', 'promedio_precio_ciudad', 'anio', 'promedio_id_zona', \n",
    "                 'promedio_precio_tipo_propiedad', 'count_id_zona', 'count_ciudad', 'puntaje', \n",
    "                 'count_tipo_propiedad_ciudad', 'promedio_precio_tipo_propiedad_ciudad_gen',\n",
    "                 'dias_desde_datos','meses_desde_datos','porcentaje_metros','distancia_ciudad_centrica', \n",
    "                 'clustering_label', 'distancia_centro_mexico'\n",
    "                ]\n",
    "\n",
    "features += features_test\n",
    "\n",
    "\n",
    "params_2nd = {'bagging_fraction': 0.8999882607358867,\n",
    " 'bagging_freq': int(95.0),\n",
    " 'feature_fraction': 0.2570109385381975,\n",
    " 'learning_rate': 0.13601832720254403,\n",
    " 'max_depth': int(26.0),\n",
    " 'num_leaves': int(175.0),\n",
    " 'test_size': 0.08363501292068126,\n",
    " 'boosting_type': 'dart',\n",
    " 'num_boost_round': 1200,\n",
    " 'objective': 'regression',\n",
    " 'metric': 'mae'}\n",
    "\n",
    "lgb_m_2nd = LightGBMWrapper(**params_2nd)\n",
    "lgb_m_2nd.fit(utils.filtrar_features(df_train_s, features + features_stacking), df_train['precio'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_s['target'] = lgb_m_2nd.predict(utils.filtrar_features(df_test_s, features + features_stacking))\n",
    "# df_test_s = utils.pesificar_df(df_test_s, col_precio_in='target', col_precio_out='target')\n",
    "df_test_s[['id', 'target']].to_csv('respuesta46.csv', index = False)\n",
    "\n",
    "# print(f'MAE Stacking-full: {utils.MAE(df_test_s[\"precio\"].values, df_test_s[\"target\"].values)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediccion solo con features de stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/Documents/FIUBA/Datos/.venv/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    }
   ],
   "source": [
    "# params_2nd = {'bagging_fraction': 0.8924398062087346,\n",
    "#  'bagging_freq': int(36.0),\n",
    "#  'feature_fraction': 0.16167385124183287,\n",
    "#  'learning_rate': 0.054693418899570134,\n",
    "#  'max_depth': int(4.0),\n",
    "#  'num_leaves': int(93.0),\n",
    "#  'objective': 'regression',\n",
    "#  'boosting_type': 'gbdt',\n",
    "#  'metric': 'mae'}\n",
    "\n",
    "params_2nd = {'bagging_fraction': 0.8243831977099841,\n",
    " 'bagging_freq': int(10.0),\n",
    " 'feature_fraction': 0.9228324501365147,\n",
    " 'learning_rate': 0.050664243951241736,\n",
    " 'max_depth': int(3.0),\n",
    " 'num_leaves': int(78.0),\n",
    " 'objective': 'regression',\n",
    " 'boosting_type': 'dart',\n",
    " 'num_boost_round': 1200,\n",
    " 'metric': 'mae'}\n",
    "\n",
    "\n",
    "lgb_m_2nd = LightGBMWrapper(**params_2nd)\n",
    "lgb_m_2nd.fit(s_train, df_train_f['precio'].values)\n",
    "\n",
    "df_test_f['target'] = lgb_m_2nd.predict(s_test)\n",
    "df_test_f[['id', 'target']].to_csv('respuesta47.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [17:21<00:00,  2.60s/it, best loss: 511432.09539443516]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.8243831977099841,\n",
       " 'bagging_freq': 10.0,\n",
       " 'feature_fraction': 0.9228324501365147,\n",
       " 'learning_rate': 0.050664243951241736,\n",
       " 'max_depth': 3.0,\n",
       " 'num_leaves': 78.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = ['stack01', 'stack02', 'stack03', 'stack04']\n",
    "\n",
    "def eval_lightgbm(args):\n",
    "    num_leaves, learning_rate, feature_fraction, bagging_fraction, bagging_freq, max_depth = args\n",
    "\n",
    "    lgb_train = lgb.Dataset(s_train, df_train['precio'].values)\n",
    "    \n",
    "    num_leaves = int(num_leaves)\n",
    "    bagging_freq = int(bagging_freq)\n",
    "    max_depth = int(max_depth)\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'metric': {'mae'}, # Si se deja vacio se toma el ideal para llegar al 'objective'\n",
    "        'num_leaves': num_leaves,\n",
    "        'learning_rate': learning_rate,\n",
    "        'feature_fraction': feature_fraction,\n",
    "        'bagging_fraction': bagging_fraction,\n",
    "        'bagging_freq': bagging_freq,\n",
    "        'max_depth': max_depth,\n",
    "        'verbose': -1,\n",
    "    }\n",
    "\n",
    "    gbm = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=250,\n",
    "                    verbose_eval=-1)\n",
    "    \n",
    "    y_pred_test = gbm.predict(s_test, num_iteration=gbm.best_iteration)\n",
    "    return utils.MAE(df_test['precio'].values, y_pred_test)\n",
    "\n",
    "space = [hp.quniform('num_leaves', 15, 130, 1), hp.uniform('learning_rate', 0.05, 0.9),\n",
    "        hp.uniform('feature_fraction', 0.90, 1), hp.uniform('bagging_fraction', 0.70, 1),\n",
    "        hp.quniform('bagging_freq', 0, 40, 1), hp.quniform('max_depth', 3, 15, 1)]\n",
    "\n",
    "hps = fmin(eval_lightgbm, space=space, algo=tpe.suggest, max_evals=400, verbose=1)\n",
    "\n",
    "display(hps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediccion con promedios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_f['target'] = np.average(s_test, axis=1)\n",
    "df_test_f[['id', 'target']].to_csv('respuesta44.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Stacking only: 519840.412728738\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "y_pred_test = mode(s_test, axis=1)[0]\n",
    "print(f\"MAE Stacking only: {utils.MAE(y_pred_test, df_test_f['precio'].values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Stacking only: 502391.48180612386\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_pred_test = np.average(s_test, axis=1)\n",
    "print(f\"MAE Stacking only: {utils.MAE(y_pred_test, df_test_f['precio'].values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 497375.416620218\n",
       " hess_inv: array([[ 7.64527106e-05, -6.84363088e-05,  5.18102889e-04,\n",
       "         1.30966978e-03],\n",
       "       [-6.84363088e-05,  6.84021400e-05, -4.91721763e-04,\n",
       "        -1.17091868e-03],\n",
       "       [ 5.18102889e-04, -4.91721763e-04,  4.00738244e-03,\n",
       "         9.31303274e-03],\n",
       "       [ 1.30966978e-03, -1.17091868e-03,  9.31303274e-03,\n",
       "         2.30019160e-02]])\n",
       "      jac: array([5.8359375 , 2.96484375, 0.3671875 , 1.5859375 ])\n",
       "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
       "     nfev: 702\n",
       "      nit: 15\n",
       "     njev: 115\n",
       "   status: 2\n",
       "  success: False\n",
       "        x: array([ 0.40410989, -0.16217251,  2.30245548,  0.39816892])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def mae_res(weights):\n",
    "    y_pred_test = np.average(s_test, weights=weights, axis=1)\n",
    "    return utils.MAE(y_pred_test, df_test_f['precio'].values)\n",
    "\n",
    "x0 = [1] * len(s_test.T)\n",
    "minimize(mae_res, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 497375.4169548869\n",
       "     jac: array([-0.58207661, -0.83819032, -0.23283064,  1.65309757])\n",
       " message: 'Optimization terminated successfully.'\n",
       "    nfev: 425\n",
       "     nit: 5\n",
       " success: True\n",
       "       x: array([ 0.49248228, -0.19790651,  2.80637364,  0.48548681])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize, differential_evolution\n",
    "\n",
    "def mae_res(weights):\n",
    "    y_pred_test = np.average(s_test, weights=weights, axis=1)\n",
    "    return utils.MAE(y_pred_test, df_test_f['precio'].values)\n",
    "\n",
    "x0 = [(-3, 4)] * len(s_test.T)\n",
    "differential_evolution(mae_res, bounds=x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_s[['id'] + features_stacking].to_csv('data/stacking5_test.csv', index=False)\n",
    "df_train_s[['id'] + features_stacking].to_csv('data/stacking5_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class BlendingTransformer():\n",
    "\n",
    "    def __init__(self, models, model_blend, val_split=0.10, verbose=True):\n",
    "        self.models = models\n",
    "        self.val_split = val_split\n",
    "        self.model_blend = model_blend\n",
    "        self.verbose = verbose\n",
    "\n",
    "        \n",
    "    def simple_fit(self, x, y):\n",
    "        x_cat = FeatureSelector(['provincia', 'intervalo_metros_cubiertos']).transform(x)\n",
    "        \n",
    "        x_train, x_eval, y_train, y_eval = train_test_split(x, y, \n",
    "                                                            test_size=self.val_split, \n",
    "                                                            stratify=x_cat)\n",
    "\n",
    "        self._fit_first(x_train, y_train)\n",
    "        self._fit_second(x_eval, y_eval)\n",
    "        \n",
    "\n",
    "\n",
    "    def fit(self, x, y, x_test):\n",
    "\n",
    "        x_cat = FeatureSelector(['provincia', 'intervalo_metros_cubiertos']).transform(x)\n",
    "        \n",
    "        x_train, x_eval, y_train, y_eval = train_test_split(x, y, \n",
    "                                                            test_size=self.val_split, \n",
    "                                                            stratify=x_cat)\n",
    "\n",
    "        self._fit_first(x_train, y_train)\n",
    "        self._fit_second(x_eval, y_eval)\n",
    "\n",
    "        self._fit_first(x, y)\n",
    "        y_test = self._predict_second(x_test)\n",
    "\n",
    "        x = np.concatenate((x, x_test))\n",
    "        y = np.concatenate((y, y_test))\n",
    "        x_cat = FeatureSelector(['provincia', 'intervalo_metros_cubiertos']).transform(x)\n",
    "\n",
    "        x_train, x_eval, y_train, y_eval = train_test_split(x, y, \n",
    "                                                            test_size=self.val_split, \n",
    "                                                            stratify=x_cat)\n",
    "        self._fit_first(x_train, y_train)\n",
    "        self._fit_second(x_eval, y_eval)\n",
    "\n",
    "        self._fit_first(x, y)\n",
    "\n",
    "\n",
    "    def _fit_first(self, x, y):\n",
    "\n",
    "        for i in range(len(self.models)):   \n",
    "            if self.verbose:\n",
    "                print(f'Fitting {i}...')\n",
    "            self.models[i] = clone(self.models[i])\n",
    "            self.models[i].fit(x, y)\n",
    "\n",
    "\n",
    "    def _fit_second(self, x, y):\n",
    "        if self.verbose:\n",
    "            print(f'Fitting blender...')\n",
    "        self.model_blend = clone(self.model_blend)\n",
    "        self.model_blend.fit(self._predict_first(x), y)\n",
    "\n",
    "\n",
    "            \n",
    "    def _predict_first(self, x):\n",
    "        preds = []\n",
    "        for i in range(len(self.models)):\n",
    "            y_pred = self.models[i].predict(x)\n",
    "            preds.append(y_pred)\n",
    "\n",
    "        return np.array(preds).T\n",
    "\n",
    "\n",
    "    def _predict_second(self, x):\n",
    "        return self.model_blend.predict(self._predict_first(x))\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self._predict_second(x)\n",
    "    \n",
    "    \n",
    "modelos = [\n",
    "           lgb_m,\n",
    "           catboost_m, \n",
    "           forest_m,\n",
    "           xgb_m\n",
    "          ]\n",
    "\n",
    "params_2nd = {'bagging_fraction': 0.8243831977099841,\n",
    " 'bagging_freq': int(10.0),\n",
    " 'feature_fraction': 0.9228324501365147,\n",
    " 'learning_rate': 0.050664243951241736,\n",
    " 'max_depth': int(3.0),\n",
    " 'num_leaves': int(78.0),\n",
    " 'objective': 'regression',\n",
    " 'boosting_type': 'dart',\n",
    " 'num_boost_round': 1500,\n",
    " 'metric': 'mae'}\n",
    "\n",
    "lgb_m_2nd = LightGBMWrapper(**params_2nd)\n",
    "\n",
    "blend = BlendingTransformer(modelos, lgb_m_2nd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 0...\n",
      "Fitting 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/javier/Documents/FIUBA/Datos/.venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-12-ff06219c7e23>\", line 1, in <module>\n",
      "    blend.simple_fit(df_train_f.drop('precio', axis=1).values, df_train_f['precio'].values)\n",
      "  File \"<ipython-input-11-5f1cad563535>\", line 21, in simple_fit\n",
      "    self._fit_first(x_train, y_train)\n",
      "  File \"<ipython-input-11-5f1cad563535>\", line 59, in _fit_first\n",
      "    self.models[i].fit(x, y)\n",
      "  File \"/home/javier/Documents/FIUBA/Datos/.venv/lib/python3.7/site-packages/sklearn/pipeline.py\", line 356, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params)\n",
      "  File \"<ipython-input-10-1c9d9daa278e>\", line 9, in fit\n",
      "    return super(CatBoostWrapper, self).fit(x, y, cat_features=cat_features)\n",
      "  File \"/home/javier/Documents/FIUBA/Datos/.venv/lib/python3.7/site-packages/catboost/core.py\", line 4239, in fit\n",
      "    save_snapshot, snapshot_file, snapshot_interval, init_model)\n",
      "  File \"/home/javier/Documents/FIUBA/Datos/.venv/lib/python3.7/site-packages/catboost/core.py\", line 1690, in _fit\n",
      "    train_params[\"init_model\"]\n",
      "  File \"/home/javier/Documents/FIUBA/Datos/.venv/lib/python3.7/site-packages/catboost/core.py\", line 1225, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 3836, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 3882, in _catboost._CatBoost._train\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/javier/Documents/FIUBA/Datos/.venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/javier/Documents/FIUBA/Datos/.venv/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/javier/Documents/FIUBA/Datos/.venv/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/javier/Documents/FIUBA/Datos/.venv/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/usr/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/usr/lib/python3.7/posixpath.py\", line 414, in _joinrealpath\n",
      "    while rest:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "blend.simple_fit(df_train_f.drop('precio', axis=1).values, df_train_f['precio'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend.fit(df_train_f.drop('precio', axis=1).values, \n",
    "                          df_train_f['precio'].values, df_test_f.drop('precio', axis=1, errors='ignore').values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_s = df_test_f.copy()\n",
    "df_test_s['target'] = blend.predict(df_test_s.values)\n",
    "# df_test_s = utils.pesificar_df(df_test_s, col_precio_in='target', col_precio_out='target')\n",
    "df_test_s[['id', 'target']].to_csv('respuesta49.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
